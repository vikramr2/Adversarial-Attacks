{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fast Gradient Sign Attack\n",
    "\n",
    "This notebook implements Fast Gradient Sign Attack."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import print_function\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torchvision import datasets, transforms\n",
    "import torchvision.models as models\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import time\n",
    "import copy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Under Attack\n",
    "\n",
    "Now, we want to get our model under attack. Let's use a pretrained ResNet on the ImageNet Dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CUDA Available:  True\n"
     ]
    }
   ],
   "source": [
    "print(\"CUDA Available: \",torch.cuda.is_available())\n",
    "device = torch.device(\"cuda\" if (torch.cuda.is_available()) else \"cpu\")\n",
    "\n",
    "model = models.resnet18(True).to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we'll get our data from Imagenette."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_dataset(data_type):\n",
    "    data_path = 'imagenette/' + data_type + '/'\n",
    "    val_dataset = datasets.ImageFolder(\n",
    "        root=data_path,\n",
    "        transform=transforms.ToTensor()\n",
    "    )\n",
    "    val_loader = torch.utils.data.DataLoader(\n",
    "        val_dataset,\n",
    "        batch_size=1,\n",
    "        num_workers=0,\n",
    "        shuffle=True\n",
    "    )\n",
    "    return val_loader\n",
    "\n",
    "test_loader = load_dataset('val')\n",
    "train_loader = load_dataset('train')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First we'll fine-tune the AlexNet so that it can analyze data from Imagenette rather than ImageNet."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_classes = 10\n",
    "batch_size = 4\n",
    "num_epochs = 15\n",
    "feature_extract = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(model, dataloaders, criterion, optimizer, num_epochs=25, is_inception=False):\n",
    "    since = time.time()\n",
    "\n",
    "    val_acc_history = []\n",
    "\n",
    "    best_model_wts = copy.deepcopy(model.state_dict())\n",
    "    best_acc = 0.0\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        print('Epoch {}/{}'.format(epoch, num_epochs - 1))\n",
    "        print('-' * 10)\n",
    "\n",
    "        # Each epoch has a training and validation phase\n",
    "        for phase in ['train', 'val']:\n",
    "            if phase == 'train':\n",
    "                model.train()  # Set model to training mode\n",
    "            else:\n",
    "                model.eval()   # Set model to evaluate mode\n",
    "\n",
    "            running_loss = 0.0\n",
    "            running_corrects = 0\n",
    "\n",
    "            # Iterate over data.\n",
    "            for inputs, labels in dataloaders[phase]:\n",
    "                inputs = inputs.to(device)\n",
    "                labels = labels.to(device)\n",
    "\n",
    "                # zero the parameter gradients\n",
    "                optimizer.zero_grad()\n",
    "\n",
    "                # forward\n",
    "                # track history if only in train\n",
    "                with torch.set_grad_enabled(phase == 'train'):\n",
    "                    # Get model outputs and calculate loss\n",
    "                    # Special case for inception because in training it has an auxiliary output. In train\n",
    "                    #   mode we calculate the loss by summing the final output and the auxiliary output\n",
    "                    #   but in testing we only consider the final output.\n",
    "                    if is_inception and phase == 'train':\n",
    "                        # From https://discuss.pytorch.org/t/how-to-optimize-inception-model-with-auxiliary-classifiers/7958\n",
    "                        outputs, aux_outputs = model(inputs)\n",
    "                        loss1 = criterion(outputs, labels)\n",
    "                        loss2 = criterion(aux_outputs, labels)\n",
    "                        loss = loss1 + 0.4*loss2\n",
    "                    else:\n",
    "                        outputs = model(inputs)\n",
    "                        loss = criterion(outputs, labels)\n",
    "\n",
    "                    _, preds = torch.max(outputs, 1)\n",
    "\n",
    "                    # backward + optimize only if in training phase\n",
    "                    if phase == 'train':\n",
    "                        loss.backward()\n",
    "                        optimizer.step()\n",
    "\n",
    "                # statistics\n",
    "                running_loss += loss.item() * inputs.size(0)\n",
    "                running_corrects += torch.sum(preds == labels.data)\n",
    "\n",
    "            epoch_loss = running_loss / len(dataloaders[phase].dataset)\n",
    "            epoch_acc = running_corrects.double() / len(dataloaders[phase].dataset)\n",
    "\n",
    "            print('{} Loss: {:.4f} Acc: {:.4f}'.format(phase, epoch_loss, epoch_acc))\n",
    "\n",
    "            # deep copy the model\n",
    "            if phase == 'val' and epoch_acc > best_acc:\n",
    "                best_acc = epoch_acc\n",
    "                best_model_wts = copy.deepcopy(model.state_dict())\n",
    "            if phase == 'val':\n",
    "                val_acc_history.append(epoch_acc)\n",
    "\n",
    "        print()\n",
    "\n",
    "    time_elapsed = time.time() - since\n",
    "    print('Training complete in {:.0f}m {:.0f}s'.format(time_elapsed // 60, time_elapsed % 60))\n",
    "    print('Best val Acc: {:4f}'.format(best_acc))\n",
    "\n",
    "    # load best model weights\n",
    "    model.load_state_dict(best_model_wts)\n",
    "    return model, val_acc_history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def set_parameter_requires_grad(model, feature_extracting):\n",
    "    if feature_extracting:\n",
    "        for param in model.parameters():\n",
    "            param.requires_grad = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "set_parameter_requires_grad(model, False)\n",
    "num_ftrs = model.fc.in_features\n",
    "model.fc = nn.Linear(num_ftrs, num_classes)\n",
    "input_size = 224"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have training code, but now we need to change the last layer of our architecture from an FC>10 to an FC10 since the Imagenette dataset only has 10 classes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Params:\n",
      "\t conv1.weight\n",
      "\t bn1.weight\n",
      "\t bn1.bias\n",
      "\t layer1.0.conv1.weight\n",
      "\t layer1.0.bn1.weight\n",
      "\t layer1.0.bn1.bias\n",
      "\t layer1.0.conv2.weight\n",
      "\t layer1.0.bn2.weight\n",
      "\t layer1.0.bn2.bias\n",
      "\t layer1.1.conv1.weight\n",
      "\t layer1.1.bn1.weight\n",
      "\t layer1.1.bn1.bias\n",
      "\t layer1.1.conv2.weight\n",
      "\t layer1.1.bn2.weight\n",
      "\t layer1.1.bn2.bias\n",
      "\t layer2.0.conv1.weight\n",
      "\t layer2.0.bn1.weight\n",
      "\t layer2.0.bn1.bias\n",
      "\t layer2.0.conv2.weight\n",
      "\t layer2.0.bn2.weight\n",
      "\t layer2.0.bn2.bias\n",
      "\t layer2.0.downsample.0.weight\n",
      "\t layer2.0.downsample.1.weight\n",
      "\t layer2.0.downsample.1.bias\n",
      "\t layer2.1.conv1.weight\n",
      "\t layer2.1.bn1.weight\n",
      "\t layer2.1.bn1.bias\n",
      "\t layer2.1.conv2.weight\n",
      "\t layer2.1.bn2.weight\n",
      "\t layer2.1.bn2.bias\n",
      "\t layer3.0.conv1.weight\n",
      "\t layer3.0.bn1.weight\n",
      "\t layer3.0.bn1.bias\n",
      "\t layer3.0.conv2.weight\n",
      "\t layer3.0.bn2.weight\n",
      "\t layer3.0.bn2.bias\n",
      "\t layer3.0.downsample.0.weight\n",
      "\t layer3.0.downsample.1.weight\n",
      "\t layer3.0.downsample.1.bias\n",
      "\t layer3.1.conv1.weight\n",
      "\t layer3.1.bn1.weight\n",
      "\t layer3.1.bn1.bias\n",
      "\t layer3.1.conv2.weight\n",
      "\t layer3.1.bn2.weight\n",
      "\t layer3.1.bn2.bias\n",
      "\t layer4.0.conv1.weight\n",
      "\t layer4.0.bn1.weight\n",
      "\t layer4.0.bn1.bias\n",
      "\t layer4.0.conv2.weight\n",
      "\t layer4.0.bn2.weight\n",
      "\t layer4.0.bn2.bias\n",
      "\t layer4.0.downsample.0.weight\n",
      "\t layer4.0.downsample.1.weight\n",
      "\t layer4.0.downsample.1.bias\n",
      "\t layer4.1.conv1.weight\n",
      "\t layer4.1.bn1.weight\n",
      "\t layer4.1.bn1.bias\n",
      "\t layer4.1.conv2.weight\n",
      "\t layer4.1.bn2.weight\n",
      "\t layer4.1.bn2.bias\n",
      "\t fc.weight\n",
      "\t fc.bias\n"
     ]
    }
   ],
   "source": [
    "model = model.to(device)\n",
    "\n",
    "params = model.parameters()\n",
    "print('Params:')\n",
    "\n",
    "for name,param in model.named_parameters():\n",
    "    if param.requires_grad:\n",
    "        print('\\t',name)\n",
    "\n",
    "optimizer_ft = optim.AdamW(params, lr=0.001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'train': <torch.utils.data.dataloader.DataLoader object at 0x7f51dc849e80>, 'val': <torch.utils.data.dataloader.DataLoader object at 0x7f51dc849e80>}\n",
      "Epoch 0/14\n",
      "----------\n",
      "train Loss: 2.3750 Acc: 0.1020\n",
      "val Loss: 2.4861 Acc: 0.1009\n",
      "\n",
      "Epoch 1/14\n",
      "----------\n",
      "train Loss: 2.2884 Acc: 0.1299\n",
      "val Loss: 2.5787 Acc: 0.0910\n",
      "\n",
      "Epoch 2/14\n",
      "----------\n",
      "train Loss: 2.1782 Acc: 0.2077\n",
      "val Loss: 2.3205 Acc: 0.1463\n",
      "\n",
      "Epoch 3/14\n",
      "----------\n",
      "train Loss: 1.8051 Acc: 0.3744\n",
      "val Loss: 2.2996 Acc: 0.2193\n",
      "\n",
      "Epoch 4/14\n",
      "----------\n",
      "train Loss: 1.5365 Acc: 0.4799\n",
      "val Loss: 2.1773 Acc: 0.3346\n",
      "\n",
      "Epoch 5/14\n",
      "----------\n",
      "train Loss: 1.3186 Acc: 0.5629\n",
      "val Loss: 3.6389 Acc: 0.3132\n",
      "\n",
      "Epoch 6/14\n",
      "----------\n",
      "train Loss: 1.1082 Acc: 0.6368\n",
      "val Loss: 3.1169 Acc: 0.3797\n",
      "\n",
      "Epoch 7/14\n",
      "----------\n",
      "train Loss: 0.9218 Acc: 0.7011\n",
      "val Loss: 2.8658 Acc: 0.3823\n",
      "\n",
      "Epoch 8/14\n",
      "----------\n",
      "train Loss: 0.7627 Acc: 0.7571\n",
      "val Loss: 2.7358 Acc: 0.4126\n",
      "\n",
      "Epoch 9/14\n",
      "----------\n",
      "train Loss: 0.6253 Acc: 0.7967\n",
      "val Loss: 4.7137 Acc: 0.4078\n",
      "\n",
      "Epoch 10/14\n",
      "----------\n",
      "train Loss: 0.4973 Acc: 0.8400\n",
      "val Loss: 6.2600 Acc: 0.3698\n",
      "\n",
      "Epoch 11/14\n",
      "----------\n",
      "train Loss: 0.4076 Acc: 0.8673\n",
      "val Loss: 4.2440 Acc: 0.4158\n",
      "\n",
      "Epoch 12/14\n",
      "----------\n",
      "train Loss: 0.3334 Acc: 0.8910\n",
      "val Loss: 5.0231 Acc: 0.4351\n",
      "\n",
      "Epoch 13/14\n",
      "----------\n",
      "train Loss: 0.2908 Acc: 0.9075\n",
      "val Loss: 13.7726 Acc: 0.3411\n",
      "\n",
      "Epoch 14/14\n",
      "----------\n",
      "train Loss: 0.2645 Acc: 0.9190\n",
      "val Loss: 4.1702 Acc: 0.4751\n",
      "\n",
      "Training complete in 148m 26s\n",
      "Best val Acc: 0.475129\n"
     ]
    }
   ],
   "source": [
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "dataloader_dict = {x: train_loader for x in ['train', 'val']}\n",
    "print(dataloader_dict)\n",
    "\n",
    "model, hist = train_model(model, dataloader_dict, criterion, optimizer_ft, num_epochs=num_epochs, is_inception=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Running FGSM\n",
    "\n",
    "Proceeding with method of attack."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fgsm_attack(image, epsilon, data_grad):\n",
    "    # Collect the element-wise sign of the data gradient\n",
    "    sign_data_grad = data_grad.sign()\n",
    "    # Create the perturbed image by adjusting each pixel of the input image\n",
    "    perturbed_image = image + epsilon*sign_data_grad\n",
    "    # Adding clipping to maintain [0,1] range\n",
    "    perturbed_image = torch.clamp(perturbed_image, 0, 1)\n",
    "    # Return the perturbed image\n",
    "    return perturbed_image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test( model, device, test_loader, epsilon ):\n",
    "\n",
    "    # Accuracy counter\n",
    "    correct = 0\n",
    "    adv_examples = []\n",
    "\n",
    "    # Loop over all examples in test set\n",
    "    for data, target in test_loader:\n",
    "\n",
    "        # Send the data and label to the device\n",
    "        data, target = data.to(device), target.to(device)\n",
    "\n",
    "        # Set requires_grad attribute of tensor. Important for Attack\n",
    "        data.requires_grad = True\n",
    "\n",
    "        # Forward pass the data through the model\n",
    "        output = model(data)\n",
    "        init_pred = output.max(1, keepdim=True)[1] # get the index of the max log-probability\n",
    "\n",
    "        # If the initial prediction is wrong, dont bother attacking, just move on\n",
    "        if init_pred.item() != target.item():\n",
    "            continue\n",
    "\n",
    "        # Calculate the loss\n",
    "        loss = F.nll_loss(output, target)\n",
    "\n",
    "        # Zero all existing gradients\n",
    "        model.zero_grad()\n",
    "\n",
    "        # Calculate gradients of model in backward pass\n",
    "        loss.backward()\n",
    "\n",
    "        # Collect datagrad\n",
    "        data_grad = data.grad.data\n",
    "\n",
    "        # Call FGSM Attack\n",
    "        perturbed_data = fgsm_attack(data, epsilon, data_grad)\n",
    "\n",
    "        # Re-classify the perturbed image\n",
    "        output = model(perturbed_data)\n",
    "\n",
    "        # Check for success\n",
    "        final_pred = output.max(1, keepdim=True)[1] # get the index of the max log-probability\n",
    "        if final_pred.item() == target.item():\n",
    "            correct += 1\n",
    "            # Special case for saving 0 epsilon examples\n",
    "            if (epsilon == 0) and (len(adv_examples) < 5):\n",
    "                adv_ex = perturbed_data.squeeze().detach().cpu().numpy()\n",
    "                adv_examples.append( (init_pred.item(), final_pred.item(), adv_ex) )\n",
    "        else:\n",
    "            # Save some adv examples for visualization later\n",
    "            if len(adv_examples) < 5:\n",
    "                adv_ex = perturbed_data.squeeze().detach().cpu().numpy()\n",
    "                adv_examples.append( (init_pred.item(), final_pred.item(), adv_ex) )\n",
    "\n",
    "    # Calculate final accuracy for this epsilon\n",
    "    final_acc = correct/float(len(test_loader))\n",
    "    print(\"Epsilon: {}\\tTest Accuracy = {} / {} = {}\".format(epsilon, correct, len(test_loader), final_acc))\n",
    "\n",
    "    # Return the accuracy and an adversarial example\n",
    "    return final_acc, adv_examples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epsilon: 0\tTest Accuracy = 1476 / 3925 = 0.37605095541401273\n",
      "Epsilon: 0.01\tTest Accuracy = 395 / 3925 = 0.10063694267515924\n",
      "Epsilon: 0.02\tTest Accuracy = 116 / 3925 = 0.029554140127388533\n",
      "Epsilon: 0.03\tTest Accuracy = 43 / 3925 = 0.010955414012738853\n",
      "Epsilon: 0.04\tTest Accuracy = 16 / 3925 = 0.004076433121019108\n",
      "Epsilon: 0.05\tTest Accuracy = 6 / 3925 = 0.0015286624203821656\n",
      "Epsilon: 0.06\tTest Accuracy = 3 / 3925 = 0.0007643312101910828\n"
     ]
    }
   ],
   "source": [
    "epsilons = [0, .01, .02, .03, .04, .05, .06]\n",
    "\n",
    "accuracies = []\n",
    "examples = []\n",
    "\n",
    "# Run test for each epsilon\n",
    "for eps in epsilons:\n",
    "    acc, ex = test(model, device, test_loader, eps)\n",
    "    accuracies.append(acc)\n",
    "    examples.append(ex)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Analysis\n",
    "\n",
    "First let's analyze a plot of accuracy vs. our peturbing epsilon (i.e. the degree of noise applied to our image)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 360x360 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.figure(figsize=(5,5))\n",
    "plt.plot(epsilons, accuracies, \"*-\")\n",
    "plt.yticks(np.arange(0, 0.5, step=0.1))\n",
    "plt.xticks(np.arange(0, .07, step=0.05))\n",
    "plt.title(\"Accuracy vs Epsilon\")\n",
    "plt.xlabel(\"Epsilon\")\n",
    "plt.ylabel(\"Accuracy\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see that with an architecture such as ResNet, the accuracy drops heavily in the smallest increase of noise. The noise can be light to fool the neural network."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
